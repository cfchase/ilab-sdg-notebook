{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "From the terminal\n",
    " - cd /opt/app-root/src\n",
    " - git clone https://github.com/Red-Hat-AI-Innovation-Team/SDG-Research.git && cd SDG-Research\n",
    " - git checkout sdg_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../SDG-Research/requirements.txt\n",
    "!pip install ../SDG-Research/.\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads environment variables from `.env` file and sets up the input directory path.  You can add or change any environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Access the variables\n",
    "input_dir = Path(os.getenv(\"INPUT_DIR\", \"document_collection\"))\n",
    "output_dir = Path(os.getenv(\"OUTPUT_DIR\", \"sdg_demo_output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PDFs to Markdown\n",
    "\n",
    "It is possible to create seed data directly from PDF, but in order to inspect the Docling results, it's often helpful to use Markdown as an intermediate format.  Here we will convert all PDFs in the input directory (document_collection by default) to markdowns in the same location.  Afterwards, correct these markdown files if Docling has made any conversion errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_markdown(\n",
    "        conversion_results: Iterable[ConversionResult],\n",
    "        overwrite: bool = False        \n",
    "):\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "\n",
    "    for conv_res in conversion_results:\n",
    "        if conv_res.status == ConversionStatus.SUCCESS:\n",
    "            success_count += 1\n",
    "            doc_directory = conv_res.input.file.parent\n",
    "            doc_filename =  conv_res.input.file.stem\n",
    "            markdown_file = doc_directory / f\"{doc_filename}.md\"\n",
    "            \n",
    "            if overwrite or not markdown_file.exists():\n",
    "                print(f\"Exporting {markdown_file}...\")\n",
    "                with markdown_file.open(\"w\") as fp:\n",
    "                    fp.write(conv_res.legacy_document.export_to_markdown())\n",
    "            else:\n",
    "                print(f\"Skipping {markdown_file} because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Document {conv_res.input.file} failed to convert.\")\n",
    "            failure_count += 1\n",
    "    \n",
    "    return success_count, failure_count\n",
    "\n",
    "\n",
    "pdf_files = list(input_dir.rglob(\"*.pdf\"))\n",
    "doc_converter = DocumentConverter()\n",
    "conversion_results = doc_converter.convert_all(pdf_files)\n",
    "convert_to_markdown(conversion_results, overwrite=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually correct the Markdown\n",
    "\n",
    "Make sure to inspect and correct the newly created markdown files in the input directory.  They will be used (instead of the PDFs) to create the seed data for synthetic data generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Markdown to JSON\n",
    "\n",
    "Now that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Seed Data \n",
    "\n",
    "Now we need to create seed_data.jsonl for synthetic data generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(path) for path in Path(\"document_collection/md\").glob(\"*.md\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from instructlab.sdg.utils.docprocessor import DocProcessor\n",
    " \n",
    "\n",
    "def create_seed_data(\n",
    "    qna_yaml_path: Path,\n",
    "    overwrite: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create seed data from a YAML file and save it as a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        qna_yaml_path (Path): Path to the YAML file containing question-answer pairs.\n",
    "        output_jsonl_path (Path): Path to save the JSONL file.\n",
    "    \"\"\"\n",
    "    # Get the directory of the YAML file\n",
    "    yaml_dir = qna_yaml_path.parent\n",
    "    # Get all markdown files in the directory\n",
    "    md_files = list(yaml_dir.glob(\"*.md\"))\n",
    "    \n",
    "    # Check if we have any markdown files\n",
    "    if not md_files:\n",
    "        print(f\"No markdown files found in {yaml_dir}\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(md_files)} markdown files in {yaml_dir}\")\n",
    "    \n",
    "    \n",
    "    dp = DocProcessor(yaml_dir, user_config_path=qna_yaml_path)\n",
    "    seed_data = dp.get_processed_markdown_dataset([str(path) for path in md_files])\n",
    "    seed_data_path = f\"{yaml_dir}/seed_data.jsonl\"    \n",
    "    seed_data.to_json(seed_data_path, orient=\"records\", lines=True)\n",
    "    return seed_data_path\n",
    "    \n",
    "\n",
    "def create_seed_data_for_all_qna_files(input_dir: Path, overwrite: bool = True):\n",
    "    \"\"\"\n",
    "    Find all qna.yaml files in the input directory and create seed data for each one.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (Path): Directory to search for qna.yaml files.\n",
    "        overwrite (bool, optional): Whether to overwrite existing seed data files. Defaults to True.\n",
    "    \"\"\"\n",
    "    # Find all qna.yaml files in the input directory and its subdirectories\n",
    "    qna_files = list(input_dir.rglob(\"qna.yaml\"))\n",
    "    \n",
    "    if not qna_files:\n",
    "        print(f\"No qna.yaml files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(qna_files)} qna.yaml files\")\n",
    "    \n",
    "    # Process each qna.yaml file\n",
    "    for qna_file in qna_files:\n",
    "        print(f\"Processing {qna_file}\")\n",
    "        try:\n",
    "            create_seed_data(qna_file, overwrite=overwrite)\n",
    "            print(f\"Successfully created seed data for {qna_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {qna_file}: {str(e)}\")\n",
    "\n",
    "\n",
    "create_seed_data_for_all_qna_files(input_dir, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Concatenate all seed_data.jsonl files\n",
    "\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def concatenate_seed_data_files(input_dir: Path, output_path: str = \"./seed_data.jsonl\"):\n",
    "    \"\"\"\n",
    "    Find all seed_data.jsonl files in the input directory and concatenate them into a single file.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (Path): Directory to search for seed_data.jsonl files.\n",
    "        output_path (str, optional): Path of the output file. Defaults to \"./seed_data.jsonl\".\n",
    "    \n",
    "    Returns:\n",
    "        Path: Path to the combined seed data file.\n",
    "    \"\"\"\n",
    "    # Find all seed_data.jsonl files in the input directory and its subdirectories\n",
    "    seed_data_files = list(input_dir.rglob(\"seed_data.jsonl\"))\n",
    "    \n",
    "    if not seed_data_files:\n",
    "        print(f\"No seed_data.jsonl files found in {input_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(seed_data_files)} seed_data.jsonl files\")\n",
    "        \n",
    "    # Concatenate all seed data files\n",
    "    combined_data = []\n",
    "    for seed_file in seed_data_files:\n",
    "        print(f\"Processing {seed_file}\")\n",
    "        try:\n",
    "            with open(seed_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Parse each line as JSON and add to combined data\n",
    "                    combined_data.append(json.loads(line))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {seed_file}: {str(e)}\")\n",
    "    \n",
    "    # Write the combined data to the output file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in combined_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"Successfully created combined seed data file at {output_path}\")\n",
    "    print(f\"Total records: {len(combined_data)}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "seed_data_path = concatenate_seed_data_files(input_dir, Path(output_dir) / \"seed_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI Client for interacting with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.environ['MIXTRAL_URL']\n",
    "openai_api_key = os.environ['MIXTRAL_TOKEN']\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "teacher_model = client.models.list().data[0].id\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SDG\n",
    "- This will create knowledge flow from provided yaml file\n",
    "- We will run this on small dataset for demo purposes\n",
    "- For large scale generation, please use the python command provided in the next cell\n",
    "- You can analyze the generated data to ensure the quality is similar to proivded QnA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_agentic_pipeline = \"scripts/synth_knowledge1.5.yaml\"\n",
    "flow_cfg = Flow(client).get_flow_from_file(knowledge_agentic_pipeline)\n",
    "sdg = SDG(\n",
    "    [Pipeline(flow_cfg)],\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    save_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 5\n",
    "ds = load_dataset('json', data_files=seed_data_path, split='train')\n",
    "ds = ds.shuffle(seed=42).select(range(number_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory is used to save the intermediate datasets\n",
    "generated_data = sdg.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data into training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data.to_json(f\"{output_dir}/gen.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the system prompt for RHELAI 1.4.1\n",
    "system_prompt_rhelai_1_4_1 = (\n",
    "    \"I am a Red HatÂ® Instruct Model, an AI language model developed by Red Hat and IBM Research based on the granite-3.1-8b-base model. My primary role is to serve as a chat assistant.\"\n",
    ")\n",
    "precomputed_skills_path = \"1.4.1/skills.jsonl\"\n",
    "# Download the RHELAI 1.4.1 data here: https://drive.google.com/file/d/1q8Rxcat5dZxXP-LqgPSCUsyttyAn6aLJ/view?usp=sharing\n",
    "# Unzip the folder and put the path to skills.jsonl in precomputed_skills_path\n",
    "postprocess_and_save(f\"{output_dir}/gen.jsonl\", dataset_save_path=f'{output_dir}', precomputed_skills_path=precomputed_skills_path, sys_prompt=system_prompt_rhelai_1_4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise complete, training data should be located in `sdg_demo_output/phase10_train.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
